{
  "best_global_step": 840,
  "best_metric": 1.9715532064437866,
  "best_model_checkpoint": "./labellens_model\\checkpoint-840",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 840,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.07155635062611806,
      "grad_norm": 2.9285237789154053,
      "learning_rate": 9.047619047619049e-06,
      "loss": 10.3314208984375,
      "step": 20
    },
    {
      "epoch": 0.14311270125223613,
      "grad_norm": 1.347757339477539,
      "learning_rate": 1.8571428571428575e-05,
      "loss": 7.274504089355469,
      "step": 40
    },
    {
      "epoch": 0.2146690518783542,
      "grad_norm": 0.8396530151367188,
      "learning_rate": 1.997761280411478e-05,
      "loss": 4.204801559448242,
      "step": 60
    },
    {
      "epoch": 0.28622540250447226,
      "grad_norm": 0.8706497550010681,
      "learning_rate": 1.9894099164431725e-05,
      "loss": 3.060473823547363,
      "step": 80
    },
    {
      "epoch": 0.35778175313059035,
      "grad_norm": 1.2983649969100952,
      "learning_rate": 1.9749279121818235e-05,
      "loss": 2.7918710708618164,
      "step": 100
    },
    {
      "epoch": 0.4293381037567084,
      "grad_norm": 0.8558610081672668,
      "learning_rate": 1.9544050018795076e-05,
      "loss": 2.6966022491455077,
      "step": 120
    },
    {
      "epoch": 0.5008944543828264,
      "grad_norm": 0.8848749995231628,
      "learning_rate": 1.9279683508079067e-05,
      "loss": 2.5446308135986326,
      "step": 140
    },
    {
      "epoch": 0.5724508050089445,
      "grad_norm": 1.5138696432113647,
      "learning_rate": 1.8957817673093258e-05,
      "loss": 2.571881866455078,
      "step": 160
    },
    {
      "epoch": 0.6440071556350626,
      "grad_norm": 0.8859217166900635,
      "learning_rate": 1.858044687797745e-05,
      "loss": 2.4474979400634767,
      "step": 180
    },
    {
      "epoch": 0.7155635062611807,
      "grad_norm": 1.297343134880066,
      "learning_rate": 1.8149909409991063e-05,
      "loss": 2.324513053894043,
      "step": 200
    },
    {
      "epoch": 0.7871198568872988,
      "grad_norm": 0.7918754816055298,
      "learning_rate": 1.7668872990879175e-05,
      "loss": 2.4222389221191407,
      "step": 220
    },
    {
      "epoch": 0.8586762075134168,
      "grad_norm": 1.1013435125350952,
      "learning_rate": 1.7140318246977164e-05,
      "loss": 2.288319396972656,
      "step": 240
    },
    {
      "epoch": 0.9302325581395349,
      "grad_norm": 1.2344027757644653,
      "learning_rate": 1.6567520240477344e-05,
      "loss": 2.3060543060302736,
      "step": 260
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.4406750202178955,
      "learning_rate": 1.595402817629475e-05,
      "loss": 2.2476432800292967,
      "step": 280
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.218611717224121,
      "eval_runtime": 156.5902,
      "eval_samples_per_second": 2.408,
      "eval_steps_per_second": 2.408,
      "step": 280
    },
    {
      "epoch": 1.071556350626118,
      "grad_norm": 1.2464817762374878,
      "learning_rate": 1.5303643410273542e-05,
      "loss": 2.200048065185547,
      "step": 300
    },
    {
      "epoch": 1.1431127012522362,
      "grad_norm": 1.3913265466690063,
      "learning_rate": 1.4620395895000892e-05,
      "loss": 2.151904487609863,
      "step": 320
    },
    {
      "epoch": 1.2146690518783543,
      "grad_norm": 1.4112681150436401,
      "learning_rate": 1.3908519209176227e-05,
      "loss": 2.1498960494995116,
      "step": 340
    },
    {
      "epoch": 1.2862254025044724,
      "grad_norm": 1.6302006244659424,
      "learning_rate": 1.3172424325260274e-05,
      "loss": 2.190252113342285,
      "step": 360
    },
    {
      "epoch": 1.3577817531305905,
      "grad_norm": 1.2904026508331299,
      "learning_rate": 1.2416672277946375e-05,
      "loss": 2.136731719970703,
      "step": 380
    },
    {
      "epoch": 1.4293381037567083,
      "grad_norm": 1.2298157215118408,
      "learning_rate": 1.164594590280734e-05,
      "loss": 2.0966901779174805,
      "step": 400
    },
    {
      "epoch": 1.5008944543828264,
      "grad_norm": 1.2433485984802246,
      "learning_rate": 1.0865020820232469e-05,
      "loss": 2.0628658294677735,
      "step": 420
    },
    {
      "epoch": 1.5724508050089445,
      "grad_norm": 1.0256136655807495,
      "learning_rate": 1.0078735844445788e-05,
      "loss": 2.1118072509765624,
      "step": 440
    },
    {
      "epoch": 1.6440071556350626,
      "grad_norm": 1.0917127132415771,
      "learning_rate": 9.291963000958932e-06,
      "loss": 2.173082733154297,
      "step": 460
    },
    {
      "epoch": 1.7155635062611807,
      "grad_norm": 0.8969210982322693,
      "learning_rate": 8.509577338238255e-06,
      "loss": 2.0102779388427736,
      "step": 480
    },
    {
      "epoch": 1.7871198568872988,
      "grad_norm": 2.1976370811462402,
      "learning_rate": 7.736426720640948e-06,
      "loss": 1.9681526184082032,
      "step": 500
    },
    {
      "epoch": 1.8586762075134167,
      "grad_norm": 1.9364107847213745,
      "learning_rate": 6.977301789790931e-06,
      "loss": 1.9409067153930664,
      "step": 520
    },
    {
      "epoch": 1.9302325581395348,
      "grad_norm": 1.184652328491211,
      "learning_rate": 6.236906280521646e-06,
      "loss": 1.9894542694091797,
      "step": 540
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.684589147567749,
      "learning_rate": 5.519827875315824e-06,
      "loss": 2.0496740341186523,
      "step": 560
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.0080270767211914,
      "eval_runtime": 162.5228,
      "eval_samples_per_second": 2.32,
      "eval_steps_per_second": 2.32,
      "step": 560
    },
    {
      "epoch": 2.071556350626118,
      "grad_norm": 1.0990394353866577,
      "learning_rate": 4.8305097778357445e-06,
      "loss": 1.9028558731079102,
      "step": 580
    },
    {
      "epoch": 2.143112701252236,
      "grad_norm": 1.41300368309021,
      "learning_rate": 4.173223181681652e-06,
      "loss": 1.8750476837158203,
      "step": 600
    },
    {
      "epoch": 2.2146690518783543,
      "grad_norm": 1.1949790716171265,
      "learning_rate": 3.552040804969149e-06,
      "loss": 1.8799718856811523,
      "step": 620
    },
    {
      "epoch": 2.2862254025044724,
      "grad_norm": 1.3498131036758423,
      "learning_rate": 2.9708116547121333e-06,
      "loss": 1.8990716934204102,
      "step": 640
    },
    {
      "epoch": 2.3577817531305905,
      "grad_norm": 1.8260549306869507,
      "learning_rate": 2.4331371773776692e-06,
      "loss": 1.9202070236206055,
      "step": 660
    },
    {
      "epoch": 2.4293381037567086,
      "grad_norm": 1.9685428142547607,
      "learning_rate": 1.9423489433902186e-06,
      "loss": 1.8880144119262696,
      "step": 680
    },
    {
      "epoch": 2.500894454382826,
      "grad_norm": 1.4590468406677246,
      "learning_rate": 1.5014880038577485e-06,
      "loss": 1.9132770538330077,
      "step": 700
    },
    {
      "epoch": 2.5724508050089447,
      "grad_norm": 1.527503252029419,
      "learning_rate": 1.1132860474308438e-06,
      "loss": 1.9665616989135741,
      "step": 720
    },
    {
      "epoch": 2.6440071556350624,
      "grad_norm": 1.8183306455612183,
      "learning_rate": 7.80148474051794e-07,
      "loss": 1.9725217819213867,
      "step": 740
    },
    {
      "epoch": 2.715563506261181,
      "grad_norm": 1.1280648708343506,
      "learning_rate": 5.04139490473069e-07,
      "loss": 1.8658592224121093,
      "step": 760
    },
    {
      "epoch": 2.7871198568872986,
      "grad_norm": 1.3670320510864258,
      "learning_rate": 2.8696931989725565e-07,
      "loss": 1.9025497436523438,
      "step": 780
    },
    {
      "epoch": 2.8586762075134167,
      "grad_norm": 1.6809635162353516,
      "learning_rate": 1.2998360499080763e-07,
      "loss": 1.9284372329711914,
      "step": 800
    },
    {
      "epoch": 2.9302325581395348,
      "grad_norm": 1.3443162441253662,
      "learning_rate": 3.4155069933301535e-08,
      "loss": 1.8605581283569337,
      "step": 820
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.8590906858444214,
      "learning_rate": 7.749316629612757e-11,
      "loss": 1.8401796340942382,
      "step": 840
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.9715532064437866,
      "eval_runtime": 164.4271,
      "eval_samples_per_second": 2.293,
      "eval_steps_per_second": 2.293,
      "step": 840
    }
  ],
  "logging_steps": 20,
  "max_steps": 840,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.6801602029579744e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
